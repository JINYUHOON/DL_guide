{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "one-hot-encoding.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "18Oifyf9Dos7VvoipX9B8_dAWKNltGRQD",
      "authorship_tag": "ABX9TyPLN6WEho9sFe0T+zvc6qir",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JINYUHOON/DL_guide/blob/main/one_hot_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAFFUJvOOWfF"
      },
      "source": [
        "자연어 처리를 위한 딥러닝은 단어, 문장, 문단에 적용한 패턴 인식\n",
        "\n",
        "- 딥러닝 모델은 수치형 텐서만 다룰 수 있음\n",
        "- 텍스트를 수치형 텐서로 변환하는 과정을 텍스트 벡터화라고 함\n",
        "- 텍스트를 나누는 단위(단어, 문자, n-그램)을 토큰이라 함\n",
        "- 텍스트를 토큰으로 나누는 작업을 토큰화라고 함\n",
        "- 텍스트 벡터화 과정은 어떤 종류의 토큰화를 적용하고 생성된 토큰에 수치형 벡터를 연결하는 것\n",
        "- 토큰과 벡터를 연결하는 방법은 원-핫 인코딩과 토큰 임베딩(단어 임베딩) 이 있음\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8Vom9yUylLs"
      },
      "source": [
        "import numpy as np\n",
        "samples = ['The cat sat on the mat.','The dog ate my homework.']\n",
        "token_index = {}\n",
        "\n",
        "for sample in samples:\n",
        "    for word in sample.split():\n",
        "        if word not in token_index:\n",
        "            token_index[word] = len(token_index) + 1\n",
        "\n",
        "token_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWYC0skIzWjH"
      },
      "source": [
        "token_index.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3E4k6kP3w_Z"
      },
      "source": [
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values())+1))\n",
        "\n",
        "results.shape\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR77P09I4S7G"
      },
      "source": [
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i,j,index] = 1. # (sample , word(행), word(열)) 해당 위치에만 1, 나머지는 0\n",
        "\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oeqZQlx6B72"
      },
      "source": [
        "# 케라스 활용\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.','The dog ate my homework.']\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10)\n",
        "tokenizer.fit_on_texts(samples)   # 단어 인덱스를 구축\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(samples) # 문자열을 정수 인덱스의 리스트로 변환\n",
        "sequences\n",
        "\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode = 'binary')\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QsiVa_v7xMv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}